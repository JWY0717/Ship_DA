{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mmsi  ship_type   latitude   longitude    cog   sog  year  month   \n",
      "0      440051540          0  35.039909  129.062547  329.2   5.7  2023      5  \\\n",
      "5      538010219          0  34.882612  129.060553  212.2  16.1  2023      5   \n",
      "17     440051540          0  35.039909  129.062547  329.2   5.7  2023      5   \n",
      "19     440102990          0  35.099860  129.041436   42.6   8.8  2023      5   \n",
      "20     440048210         80  35.064618  129.110167   31.6   9.6  2023      5   \n",
      "...          ...        ...        ...         ...    ...   ...   ...    ...   \n",
      "52705  440135680          0  34.958493  129.193000  306.5  10.1  2023      5   \n",
      "52708  440658000         70  35.042145  129.170300  239.4   7.5  2023      5   \n",
      "52709  440761000         71  35.019416  129.126246  210.0   7.6  2023      5   \n",
      "52714  440003510          0  35.035548  129.038131   90.6  15.3  2023      5   \n",
      "52726  563156900         71  34.813431  129.000033   49.5  11.8  2023      5   \n",
      "\n",
      "       day  hour  minute  second   풍향   유향    기온    수온    풍속   유속    기압  습도  \n",
      "0       11    10      10      10  196  246  16.4  17.0  9.77  0.3  1017  82  \n",
      "5       11    10      10      10  196  246  16.4  17.0  9.77  0.3  1017  82  \n",
      "17      11    10      10      10  196  246  16.4  17.0  9.77  0.3  1017  82  \n",
      "19      11    10      10      10  196  246  16.4  17.0  9.77  0.3  1017  82  \n",
      "20      11    10      10      10  196  246  16.4  17.0  9.77  0.3  1017  82  \n",
      "...    ...   ...     ...     ...  ...  ...   ...   ...   ...  ...   ...  ..  \n",
      "52705   11    11      19      19  191  249  16.4  16.6  9.25  0.1  1017  80  \n",
      "52708   11    11      19      19  191  249  16.4  16.6  9.25  0.1  1017  80  \n",
      "52709   11    11      19      19  191  249  16.4  16.6  9.25  0.1  1017  80  \n",
      "52714   11    11      19      19  191  249  16.4  16.6  9.25  0.1  1017  80  \n",
      "52726   11    11      19      19  191  249  16.4  16.6  9.25  0.1  1017  80  \n",
      "\n",
      "[16176 rows x 20 columns]\n",
      "Epoch 1/100\n",
      "405/405 [==============================] - 4s 6ms/step - loss: 11799.0508 - val_loss: 9976.3184\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 8643.6113 - val_loss: 7474.6309\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 6547.3398 - val_loss: 5718.0767\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 5084.2690 - val_loss: 4506.5337\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4098.3804 - val_loss: 3710.3733\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3468.8198 - val_loss: 3216.2883\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3091.4033 - val_loss: 2928.5100\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2882.4910 - val_loss: 2775.6777\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2777.0210 - val_loss: 2701.7737\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2729.6716 - val_loss: 2669.9021\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2712.0242 - val_loss: 2658.3940\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2706.2261 - val_loss: 2654.6951\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.5835 - val_loss: 2653.2935\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2219 - val_loss: 2653.2371\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2197 - val_loss: 2652.9207\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2114 - val_loss: 2652.9863\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.1472 - val_loss: 2652.8647\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2158 - val_loss: 2652.9082\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.1692 - val_loss: 2652.7969\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 2s 6ms/step - loss: 2704.2151 - val_loss: 2652.8760\n",
      "Epoch 21/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2273 - val_loss: 2653.1069\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2524 - val_loss: 2653.1201\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 2s 6ms/step - loss: 2704.3789 - val_loss: 2652.8621\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2173 - val_loss: 2652.8140\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2021 - val_loss: 2653.0725\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.3013 - val_loss: 2653.0491\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2476 - val_loss: 2652.7268\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2346 - val_loss: 2652.7429\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.3540 - val_loss: 2652.8811\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.1479 - val_loss: 2652.7756\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2466 - val_loss: 2652.9080\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.3179 - val_loss: 2653.0386\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2695 - val_loss: 2652.9189\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2402 - val_loss: 2652.9880\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2019 - val_loss: 2653.2590\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2520 - val_loss: 2652.9004\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2769 - val_loss: 2652.9629\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2090 - val_loss: 2652.8218\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.2954 - val_loss: 2652.9512\n",
      "Epoch 40/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2704.4285 - val_loss: 2652.5151\n",
      "Epoch 41/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2703.3679 - val_loss: 2651.5188\n",
      "Epoch 42/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2702.7913 - val_loss: 2651.3176\n",
      "Epoch 43/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2702.2615 - val_loss: 2650.9270\n",
      "Epoch 44/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2701.4548 - val_loss: 2650.8352\n",
      "Epoch 45/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2701.2188 - val_loss: 2650.4480\n",
      "Epoch 46/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2700.5320 - val_loss: 2650.2568\n",
      "Epoch 47/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2700.4539 - val_loss: 2651.9443\n",
      "Epoch 48/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2700.5791 - val_loss: 2650.2920\n",
      "Epoch 49/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2700.0225 - val_loss: 2650.7632\n",
      "Epoch 50/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2700.2263 - val_loss: 2650.1980\n",
      "Epoch 51/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.9189 - val_loss: 2650.6638\n",
      "Epoch 52/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2700.0283 - val_loss: 2650.2104\n",
      "Epoch 53/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.8552 - val_loss: 2650.9827\n",
      "Epoch 54/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.9690 - val_loss: 2650.3650\n",
      "Epoch 55/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.7739 - val_loss: 2650.7065\n",
      "Epoch 56/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.9036 - val_loss: 2650.3579\n",
      "Epoch 57/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.6426 - val_loss: 2650.3022\n",
      "Epoch 58/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2699.5298 - val_loss: 2650.1418\n",
      "Epoch 59/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2442.0466 - val_loss: 1482.8209\n",
      "Epoch 60/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1074.4341 - val_loss: 751.9120\n",
      "Epoch 61/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 586.4745 - val_loss: 441.6921\n",
      "Epoch 62/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 349.4997 - val_loss: 273.4533\n",
      "Epoch 63/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 221.7302 - val_loss: 178.9101\n",
      "Epoch 64/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 144.2605 - val_loss: 120.7012\n",
      "Epoch 65/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 92.2229 - val_loss: 72.5749\n",
      "Epoch 66/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 62.1534 - val_loss: 47.7592\n",
      "Epoch 67/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 40.7367 - val_loss: 32.3104\n",
      "Epoch 68/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 27.3481 - val_loss: 22.8668\n",
      "Epoch 69/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 19.2281 - val_loss: 20.0237\n",
      "Epoch 70/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 13.6312 - val_loss: 11.0400\n",
      "Epoch 71/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 10.0462 - val_loss: 9.1361\n",
      "Epoch 72/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 7.5888 - val_loss: 6.3072\n",
      "Epoch 73/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 6.5507 - val_loss: 6.2703\n",
      "Epoch 74/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 7.9054 - val_loss: 5.5867\n",
      "Epoch 75/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4.5416 - val_loss: 4.6349\n",
      "Epoch 76/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4.3400 - val_loss: 5.0313\n",
      "Epoch 77/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4.5933 - val_loss: 4.3768\n",
      "Epoch 78/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3.9936 - val_loss: 5.0442\n",
      "Epoch 79/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4.1546 - val_loss: 3.7279\n",
      "Epoch 80/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3.8234 - val_loss: 6.7475\n",
      "Epoch 81/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4.0083 - val_loss: 3.5846\n",
      "Epoch 82/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 4.3947 - val_loss: 3.4464\n",
      "Epoch 83/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3.5894 - val_loss: 4.3228\n",
      "Epoch 84/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3.8786 - val_loss: 3.7266\n",
      "Epoch 85/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3.2607 - val_loss: 3.7547\n",
      "Epoch 86/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 3.6294 - val_loss: 3.4724\n",
      "Epoch 87/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2.9060 - val_loss: 3.1911\n",
      "Epoch 88/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2.9020 - val_loss: 2.8168\n",
      "Epoch 89/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2.4254 - val_loss: 3.1772\n",
      "Epoch 90/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2.5471 - val_loss: 2.0249\n",
      "Epoch 91/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2.3657 - val_loss: 1.9101\n",
      "Epoch 92/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 2.1945 - val_loss: 4.4901\n",
      "Epoch 93/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.9071 - val_loss: 1.7984\n",
      "Epoch 94/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.5956 - val_loss: 3.5935\n",
      "Epoch 95/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.9740 - val_loss: 1.1688\n",
      "Epoch 96/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.4640 - val_loss: 1.0938\n",
      "Epoch 97/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.4508 - val_loss: 1.1867\n",
      "Epoch 98/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.3348 - val_loss: 0.9945\n",
      "Epoch 99/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.2489 - val_loss: 1.1182\n",
      "Epoch 100/100\n",
      "405/405 [==============================] - 2s 5ms/step - loss: 1.4666 - val_loss: 1.0889\n",
      "102/102 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 1.0888897231293289\n",
      "{'latitude': 34.9339714050293, 'longitude': 129.0572052001953, 'predicted_cog': 234.74525451660156, 'predicted_sog': 11.7249174118042}\n"
     ]
    }
   ],
   "source": [
    "# LSTM + CNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Conv1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pyproj import Transformer\n",
    "\n",
    "import binascii\n",
    "from shapely import wkb\n",
    "import pyproj\n",
    "\n",
    "def convert_wkb_to_coordinates(wkb_string):\n",
    "    \n",
    "    # 0단계 :Check if the value is a float\n",
    "    if isinstance(wkb_string, float):\n",
    "        return None, None\n",
    "    \n",
    "    # 1단계: 16진수 문자열을 이진 형식으로 디코딩\n",
    "    binary_string = binascii.unhexlify(wkb_string)\n",
    "\n",
    "    # 2단계: 이진 형식 파싱 \n",
    "    geometry = wkb.loads(binary_string)\n",
    "\n",
    "    # 3단계: 좌표를 위도와 경도로 변환\n",
    "    if geometry and hasattr(geometry, 'wkt'):\n",
    "        # Well-Known Text (WKT) 형식으로 변환\n",
    "        wkt = geometry.wkt\n",
    "\n",
    "        # 좌표 시스템 변환 정의\n",
    "        #!crs_from = pyproj.Proj(init='epsg:3857')\n",
    "        #!crs_to = pyproj.Proj(init='epsg:4326')\n",
    "        transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\")\n",
    "        transformer.transform(12, 12)\n",
    "\n",
    "        # 좌표 변환 수행\n",
    "        #! transformed = pyproj.transform(crs_from, crs_to, geometry.x, geometry.y)\n",
    "        transformed = Transformer.transform(transformer,geometry.x, geometry.y)\n",
    "\n",
    "        # 위도와 경도 추출\n",
    "        latitude, longitude = transformed[0], transformed[1]\n",
    "        \n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "# CSV 파일에서 데이터 로드\n",
    "data = pd.read_csv('D:/장우영/LOCALSEARCH/Ship_DA/DA/data/FAmerge_20230607_114714.csv', encoding='ANSI')\n",
    "\n",
    "# 특정 칼럼의 조건에 따라 데이터 필터링\n",
    "filtered_data = data[data['sog'] > 3]  # 특정 칼럼과 조건을 적절히 수정해야 합니다\n",
    "\n",
    "data= filtered_data\n",
    "\n",
    "\n",
    "# 좌표 변환 적용 및 데이터프레임 업데이트\n",
    "data[['latitude', 'longitude']] = data['geom'].apply(convert_wkb_to_coordinates).apply(pd.Series)\n",
    "\n",
    "#print(data[['latitude', 'longitude']])\n",
    "\n",
    "# \"insert_time\"을 숫자로 변환\n",
    "data['year'] = pd.to_datetime(data['insert_time']).dt.year\n",
    "data['month'] = pd.to_datetime(data['insert_time']).dt.month\n",
    "data['day'] = pd.to_datetime(data['insert_time']).dt.day\n",
    "data['hour'] = pd.to_datetime(data['insert_time']).dt.hour\n",
    "data['minute'] = pd.to_datetime(data['insert_time']).dt.minute\n",
    "data['second'] = pd.to_datetime(data['insert_time']).dt.minute\n",
    "\n",
    "\n",
    "# 특성과 타겟 변수 선택\n",
    "X = data[[\"mmsi\", \"ship_type\", \"latitude\", \"longitude\", \"cog\", \"sog\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\", \"풍향\", \"유향\", \"기온\", \"수온\", \"풍속\", \"유속\", \"기압\", \"습도\"]]\n",
    "y = data[[\"latitude\", \"longitude\",\"cog\", \"sog\"]]\n",
    "\n",
    "print(X)\n",
    "\n",
    "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# MinMaxScaler를 사용하여 입력 특성 스케일 조정\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# LSTM에 맞게 입력 데이터 형태 변환\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "\n",
    "# LSTM과 CNN을 결합한 모델 생성\n",
    "model = Sequential()\n",
    "\n",
    "# Conv1D 레이어를 추가합니다. 1D 컨볼루션 연산을 수행하는 레이어로, 입력 데이터의 필터 수는 64개, 커널 크기는 3입니다. 활성화 함수로는 ReLU를 사용하고, 입력 형태는 X_train_reshaped의 shape[1]과 shape[2]에 해당합니다.\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "\n",
    "#  LSTM은 순환 신경망의 한 종류로, 입력 데이터에 대한 장기 의존성을 모델링하는 데 사용됩니다. 이 레이어의 뉴런 수는 64개    \n",
    "model.add(LSTM(64))\n",
    "\n",
    "# Dense 레이어를 추가합니다. 출력층으로 사용되며, 뉴런 수는 4개입니다. 이 뉴런은 (latitude, longitude, sog, cog)와 같은 4개의 출력 값을 예측하는 데 사용\n",
    "model.add(Dense(4))  \n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# 조기 종료를 위한 EarlyStopping 정의\n",
    "#early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "# 모델을 사용하여 예측 수행\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert predictions to a list\n",
    "y_pred_list = y_pred.tolist()\n",
    "\n",
    "# 평균 제곱 오차 계산\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Create the response JSON\n",
    "response = {\n",
    "    \"latitude\": y_pred_list[0][0],\n",
    "    \"longitude\": y_pred_list[0][1],\n",
    "    \"predicted_cog\": y_pred_list[0][2],\n",
    "    \"predicted_sog\": y_pred_list[0][3]\n",
    "}\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 26.6726 - val_loss: 0.0634\n",
      "Epoch 2/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 0.0601 - val_loss: 0.0628\n",
      "Epoch 3/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0587 - val_loss: 0.0619\n",
      "Epoch 4/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0588 - val_loss: 0.0610\n",
      "Epoch 5/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0587 - val_loss: 0.0609\n",
      "Epoch 6/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0587 - val_loss: 0.0622\n",
      "Epoch 7/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 0.0586 - val_loss: 0.0616\n",
      "Epoch 8/100\n",
      "3842/3842 [==============================] - 21s 6ms/step - loss: 0.0586 - val_loss: 0.0635\n",
      "Epoch 9/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 0.0586 - val_loss: 0.0619\n",
      "Epoch 10/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0586 - val_loss: 0.0617\n",
      "Epoch 11/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0592 - val_loss: 0.0652\n",
      "Epoch 12/100\n",
      "3842/3842 [==============================] - 21s 6ms/step - loss: 0.0583 - val_loss: 0.0611\n",
      "Epoch 13/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0587 - val_loss: 0.0611\n",
      "Epoch 14/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 0.0585 - val_loss: 0.2817\n",
      "Epoch 15/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 0.0711 - val_loss: 0.0044\n",
      "Epoch 16/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 0.0020 - val_loss: 0.0417\n",
      "Epoch 17/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 0.0106 - val_loss: 7.6808e-04\n",
      "Epoch 18/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 4.1616e-04 - val_loss: 3.7483e-04\n",
      "Epoch 19/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.2525e-04 - val_loss: 3.9863e-05\n",
      "Epoch 20/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 8.7050e-05 - val_loss: 8.8072e-05\n",
      "Epoch 21/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 6.8674e-05 - val_loss: 1.9259e-05\n",
      "Epoch 22/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 6.4571e-05 - val_loss: 2.8006e-05\n",
      "Epoch 23/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 5.3726e-05 - val_loss: 2.6586e-05\n",
      "Epoch 24/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 4.5655e-05 - val_loss: 1.5306e-05\n",
      "Epoch 25/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 3.9455e-05 - val_loss: 4.7691e-05\n",
      "Epoch 26/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 1.1164e-04 - val_loss: 2.5563e-05\n",
      "Epoch 27/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 2.7069e-05 - val_loss: 2.3846e-05\n",
      "Epoch 28/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 0.0022 - val_loss: 7.6128e-04\n",
      "Epoch 29/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 1.1861e-04 - val_loss: 7.0211e-05\n",
      "Epoch 30/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 5.0868e-05 - val_loss: 5.0431e-05\n",
      "Epoch 31/100\n",
      "3842/3842 [==============================] - 21s 6ms/step - loss: 4.1437e-05 - val_loss: 9.9544e-05\n",
      "Epoch 32/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 4.1873e-05 - val_loss: 5.1368e-06\n",
      "Epoch 33/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 3.3731e-05 - val_loss: 2.1765e-05\n",
      "Epoch 34/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 3.1999e-05 - val_loss: 6.4114e-06\n",
      "Epoch 35/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 2.8298e-05 - val_loss: 1.1186e-05\n",
      "Epoch 36/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 2.6434e-05 - val_loss: 5.7428e-06\n",
      "Epoch 37/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 2.8219e-05 - val_loss: 4.0727e-05\n",
      "Epoch 38/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 2.5419e-05 - val_loss: 5.1575e-06\n",
      "Epoch 39/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 2.2982e-05 - val_loss: 4.8989e-06\n",
      "Epoch 40/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 2.1675e-05 - val_loss: 6.1303e-05\n",
      "Epoch 41/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 2.4658e-05 - val_loss: 8.0392e-06\n",
      "Epoch 42/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 2.0742e-05 - val_loss: 5.6587e-06\n",
      "Epoch 43/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.9717e-05 - val_loss: 4.9834e-05\n",
      "Epoch 44/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.9178e-05 - val_loss: 7.4334e-06\n",
      "Epoch 45/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 2.0481e-05 - val_loss: 1.8063e-06\n",
      "Epoch 46/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.8828e-05 - val_loss: 6.0035e-05\n",
      "Epoch 47/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.8435e-05 - val_loss: 1.4328e-06\n",
      "Epoch 48/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.7034e-05 - val_loss: 5.7737e-05\n",
      "Epoch 49/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.7351e-05 - val_loss: 5.6512e-06\n",
      "Epoch 50/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 1.7983e-05 - val_loss: 2.6399e-06\n",
      "Epoch 51/100\n",
      "3842/3842 [==============================] - 21s 6ms/step - loss: 1.7405e-05 - val_loss: 4.1533e-06\n",
      "Epoch 52/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.7057e-05 - val_loss: 2.4277e-06\n",
      "Epoch 53/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5782e-05 - val_loss: 9.7540e-06\n",
      "Epoch 54/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.7409e-05 - val_loss: 1.3214e-06\n",
      "Epoch 55/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 1.6317e-05 - val_loss: 2.9294e-06\n",
      "Epoch 56/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5650e-05 - val_loss: 9.7994e-05\n",
      "Epoch 57/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 1.5749e-05 - val_loss: 2.2595e-06\n",
      "Epoch 58/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5613e-05 - val_loss: 2.7885e-06\n",
      "Epoch 59/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.4943e-05 - val_loss: 3.8576e-06\n",
      "Epoch 60/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5741e-05 - val_loss: 4.4349e-06\n",
      "Epoch 61/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5456e-05 - val_loss: 2.8778e-05\n",
      "Epoch 62/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 1.4179e-05 - val_loss: 5.9999e-06\n",
      "Epoch 63/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5134e-05 - val_loss: 3.8969e-05\n",
      "Epoch 64/100\n",
      "3842/3842 [==============================] - 21s 6ms/step - loss: 1.5658e-05 - val_loss: 5.8032e-04\n",
      "Epoch 65/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.5338e-05 - val_loss: 1.7989e-06\n",
      "Epoch 66/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 1.4161e-05 - val_loss: 6.8420e-05\n",
      "Epoch 67/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 1.4830e-05 - val_loss: 1.3453e-05\n",
      "Epoch 68/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3310e-05 - val_loss: 9.8063e-07\n",
      "Epoch 69/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3877e-05 - val_loss: 4.4042e-06\n",
      "Epoch 70/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3670e-05 - val_loss: 1.7300e-06\n",
      "Epoch 71/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3816e-05 - val_loss: 1.4478e-05\n",
      "Epoch 72/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3517e-05 - val_loss: 1.0245e-05\n",
      "Epoch 73/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3892e-05 - val_loss: 1.2429e-05\n",
      "Epoch 74/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3250e-05 - val_loss: 1.0229e-05\n",
      "Epoch 75/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3074e-05 - val_loss: 3.2905e-05\n",
      "Epoch 76/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 4.7383e-05 - val_loss: 3.3358e-06\n",
      "Epoch 77/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 9.5682e-06 - val_loss: 3.6518e-06\n",
      "Epoch 78/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3470e-05 - val_loss: 8.0822e-07\n",
      "Epoch 79/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3834e-05 - val_loss: 9.6847e-06\n",
      "Epoch 80/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3230e-05 - val_loss: 3.7339e-06\n",
      "Epoch 81/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3037e-05 - val_loss: 1.5894e-06\n",
      "Epoch 82/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3675e-05 - val_loss: 1.6383e-05\n",
      "Epoch 83/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1780e-05 - val_loss: 3.5545e-06\n",
      "Epoch 84/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3640e-05 - val_loss: 4.9600e-06\n",
      "Epoch 85/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1701e-05 - val_loss: 1.4061e-06\n",
      "Epoch 86/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.2827e-05 - val_loss: 2.5249e-05\n",
      "Epoch 87/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1938e-05 - val_loss: 1.2057e-05\n",
      "Epoch 88/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.2580e-05 - val_loss: 2.3990e-05\n",
      "Epoch 89/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1659e-05 - val_loss: 3.0105e-06\n",
      "Epoch 90/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.3327e-05 - val_loss: 4.1045e-05\n",
      "Epoch 91/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.2498e-05 - val_loss: 1.3422e-06\n",
      "Epoch 92/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1542e-05 - val_loss: 2.3587e-06\n",
      "Epoch 93/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1983e-05 - val_loss: 1.9206e-06\n",
      "Epoch 94/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1490e-05 - val_loss: 2.8390e-05\n",
      "Epoch 95/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1168e-05 - val_loss: 4.8373e-06\n",
      "Epoch 96/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 1.1371e-05 - val_loss: 4.7829e-06\n",
      "Epoch 97/100\n",
      "3842/3842 [==============================] - 22s 6ms/step - loss: 1.1616e-05 - val_loss: 2.2050e-05\n",
      "Epoch 98/100\n",
      "3842/3842 [==============================] - 21s 6ms/step - loss: 1.0952e-05 - val_loss: 2.5601e-05\n",
      "Epoch 99/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1554e-05 - val_loss: 8.0627e-06\n",
      "Epoch 100/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.1385e-05 - val_loss: 2.2609e-06\n",
      "Epoch 1/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 2520.5298 - val_loss: 0.3203\n",
      "Epoch 2/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0856 - val_loss: 0.0789\n",
      "Epoch 3/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0766 - val_loss: 0.0810\n",
      "Epoch 4/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0770 - val_loss: 0.0789\n",
      "Epoch 5/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0800\n",
      "Epoch 6/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 7/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0795\n",
      "Epoch 8/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0780 - val_loss: 0.0855\n",
      "Epoch 9/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0793\n",
      "Epoch 10/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0800\n",
      "Epoch 11/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0818\n",
      "Epoch 12/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0809\n",
      "Epoch 13/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0796\n",
      "Epoch 14/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0789\n",
      "Epoch 15/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0788\n",
      "Epoch 16/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0789\n",
      "Epoch 17/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0799\n",
      "Epoch 18/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0796\n",
      "Epoch 19/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 20/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0788\n",
      "Epoch 21/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0788\n",
      "Epoch 22/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0854\n",
      "Epoch 23/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0809\n",
      "Epoch 24/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0790\n",
      "Epoch 25/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0795\n",
      "Epoch 26/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0788\n",
      "Epoch 27/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0790\n",
      "Epoch 28/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0781 - val_loss: 0.0797\n",
      "Epoch 29/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0789\n",
      "Epoch 30/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0794\n",
      "Epoch 31/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0789\n",
      "Epoch 32/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0801\n",
      "Epoch 33/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0789\n",
      "Epoch 34/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0790\n",
      "Epoch 35/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0789\n",
      "Epoch 36/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0794\n",
      "Epoch 37/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0789\n",
      "Epoch 38/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0798\n",
      "Epoch 39/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0867\n",
      "Epoch 40/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0831\n",
      "Epoch 41/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0789\n",
      "Epoch 42/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0821\n",
      "Epoch 43/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0814\n",
      "Epoch 44/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0788\n",
      "Epoch 45/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0799\n",
      "Epoch 46/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0828\n",
      "Epoch 47/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0793\n",
      "Epoch 48/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0788\n",
      "Epoch 49/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0790\n",
      "Epoch 50/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0824\n",
      "Epoch 51/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0845\n",
      "Epoch 52/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0798\n",
      "Epoch 53/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0827\n",
      "Epoch 54/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0804\n",
      "Epoch 55/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0791\n",
      "Epoch 56/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0825\n",
      "Epoch 57/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0788\n",
      "Epoch 58/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0802\n",
      "Epoch 59/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 60/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0798\n",
      "Epoch 61/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0790\n",
      "Epoch 62/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0838\n",
      "Epoch 63/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0859\n",
      "Epoch 64/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0788\n",
      "Epoch 65/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0789\n",
      "Epoch 66/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0808\n",
      "Epoch 67/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0795\n",
      "Epoch 68/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0802\n",
      "Epoch 69/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0817\n",
      "Epoch 70/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0806\n",
      "Epoch 71/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0832\n",
      "Epoch 72/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0805\n",
      "Epoch 73/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0830\n",
      "Epoch 74/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0788\n",
      "Epoch 75/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0839\n",
      "Epoch 76/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0817\n",
      "Epoch 77/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0790\n",
      "Epoch 78/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0791\n",
      "Epoch 79/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0789\n",
      "Epoch 80/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0810\n",
      "Epoch 81/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0788\n",
      "Epoch 82/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 0.0801\n",
      "Epoch 83/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0798\n",
      "Epoch 84/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0788\n",
      "Epoch 85/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0812\n",
      "Epoch 86/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0791\n",
      "Epoch 87/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0780 - val_loss: 0.0789\n",
      "Epoch 88/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0807\n",
      "Epoch 89/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0788\n",
      "Epoch 90/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0788\n",
      "Epoch 91/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0807\n",
      "Epoch 92/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0791\n",
      "Epoch 93/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0795\n",
      "Epoch 94/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0803\n",
      "Epoch 95/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0812\n",
      "Epoch 96/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0778 - val_loss: 0.0803\n",
      "Epoch 97/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0805\n",
      "Epoch 98/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0836\n",
      "Epoch 99/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0828\n",
      "Epoch 100/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0779 - val_loss: 0.0788\n",
      "Epoch 1/100\n",
      "3842/3842 [==============================] - 21s 5ms/step - loss: 16977.3457 - val_loss: 10451.3516\n",
      "Epoch 2/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 10460.3594 - val_loss: 10387.8770\n",
      "Epoch 3/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 7983.4316 - val_loss: 1007.0734\n",
      "Epoch 4/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 270.1762 - val_loss: 29.8892\n",
      "Epoch 5/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 18.0850 - val_loss: 18.4070\n",
      "Epoch 6/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 7.9723 - val_loss: 3.7738\n",
      "Epoch 7/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 5.9186 - val_loss: 5.1937\n",
      "Epoch 8/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 5.2192 - val_loss: 2.4496\n",
      "Epoch 9/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 4.1528 - val_loss: 4.7018\n",
      "Epoch 10/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 2.3847 - val_loss: 2.2161\n",
      "Epoch 11/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.2971 - val_loss: 1.5094\n",
      "Epoch 12/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.2229 - val_loss: 2.7974\n",
      "Epoch 13/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 1.0154 - val_loss: 4.8595\n",
      "Epoch 14/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.8785 - val_loss: 4.1711\n",
      "Epoch 15/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.8784 - val_loss: 1.5659\n",
      "Epoch 16/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.6718 - val_loss: 1.1640\n",
      "Epoch 17/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.7099 - val_loss: 1.2119\n",
      "Epoch 18/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.5876 - val_loss: 1.2115\n",
      "Epoch 19/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.6290 - val_loss: 1.7573\n",
      "Epoch 20/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.5075 - val_loss: 0.9908\n",
      "Epoch 21/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4863 - val_loss: 2.5822\n",
      "Epoch 22/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4784 - val_loss: 0.7602\n",
      "Epoch 23/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4457 - val_loss: 0.7706\n",
      "Epoch 24/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4246 - val_loss: 0.7480\n",
      "Epoch 25/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4468 - val_loss: 6.9710\n",
      "Epoch 26/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4216 - val_loss: 0.8308\n",
      "Epoch 27/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4190 - val_loss: 1.9726\n",
      "Epoch 28/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.3232 - val_loss: 0.6776\n",
      "Epoch 29/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.4489 - val_loss: 0.9775\n",
      "Epoch 30/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.3373 - val_loss: 0.7085\n",
      "Epoch 31/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.3218 - val_loss: 0.9719\n",
      "Epoch 32/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2897 - val_loss: 1.4633\n",
      "Epoch 33/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.3018 - val_loss: 0.7756\n",
      "Epoch 34/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2941 - val_loss: 0.6985\n",
      "Epoch 35/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2794 - val_loss: 0.8470\n",
      "Epoch 36/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2447 - val_loss: 0.7025\n",
      "Epoch 37/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2518 - val_loss: 0.9377\n",
      "Epoch 38/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2913 - val_loss: 1.1367\n",
      "Epoch 39/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2480 - val_loss: 0.8517\n",
      "Epoch 40/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2188 - val_loss: 0.8818\n",
      "Epoch 41/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2436 - val_loss: 0.7432\n",
      "Epoch 42/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2138 - val_loss: 1.2748\n",
      "Epoch 43/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1878 - val_loss: 1.2892\n",
      "Epoch 44/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2322 - val_loss: 0.7543\n",
      "Epoch 45/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1745 - val_loss: 0.7774\n",
      "Epoch 46/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1920 - val_loss: 0.8334\n",
      "Epoch 47/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1978 - val_loss: 0.8596\n",
      "Epoch 48/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 0.1746 - val_loss: 0.7715\n",
      "Epoch 49/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1800 - val_loss: 0.8589\n",
      "Epoch 50/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1805 - val_loss: 0.8684\n",
      "Epoch 51/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1800 - val_loss: 1.0088\n",
      "Epoch 52/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1678 - val_loss: 1.2101\n",
      "Epoch 53/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.2263 - val_loss: 0.8224\n",
      "Epoch 54/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1221 - val_loss: 0.8649\n",
      "Epoch 55/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1424 - val_loss: 0.8583\n",
      "Epoch 56/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1391 - val_loss: 0.8333\n",
      "Epoch 57/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1411 - val_loss: 0.9647\n",
      "Epoch 58/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1599 - val_loss: 0.8831\n",
      "Epoch 59/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1313 - val_loss: 0.9858\n",
      "Epoch 60/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1385 - val_loss: 0.8817\n",
      "Epoch 61/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1292 - val_loss: 1.0804\n",
      "Epoch 62/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1203 - val_loss: 0.9317\n",
      "Epoch 63/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1187 - val_loss: 1.5557\n",
      "Epoch 64/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1166 - val_loss: 0.9694\n",
      "Epoch 65/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1124 - val_loss: 0.8962\n",
      "Epoch 66/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1094 - val_loss: 0.9164\n",
      "Epoch 67/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1247 - val_loss: 0.9282\n",
      "Epoch 68/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0975 - val_loss: 0.9613\n",
      "Epoch 69/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1146 - val_loss: 1.0561\n",
      "Epoch 70/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1047 - val_loss: 1.1226\n",
      "Epoch 71/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0903 - val_loss: 1.0141\n",
      "Epoch 72/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1062 - val_loss: 0.9501\n",
      "Epoch 73/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0974 - val_loss: 1.1812\n",
      "Epoch 74/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.1061 - val_loss: 1.0007\n",
      "Epoch 75/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0892 - val_loss: 1.8276\n",
      "Epoch 76/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0939 - val_loss: 1.1152\n",
      "Epoch 77/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0824 - val_loss: 1.0186\n",
      "Epoch 78/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0869 - val_loss: 0.9879\n",
      "Epoch 79/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0784 - val_loss: 1.1116\n",
      "Epoch 80/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0796 - val_loss: 1.0790\n",
      "Epoch 81/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0776 - val_loss: 1.0324\n",
      "Epoch 82/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0839 - val_loss: 1.1596\n",
      "Epoch 83/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0811 - val_loss: 1.0654\n",
      "Epoch 84/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0796 - val_loss: 1.0181\n",
      "Epoch 85/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0775 - val_loss: 1.0404\n",
      "Epoch 86/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0729 - val_loss: 1.1666\n",
      "Epoch 87/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0694 - val_loss: 1.0017\n",
      "Epoch 88/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0740 - val_loss: 1.0095\n",
      "Epoch 89/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0763 - val_loss: 1.0157\n",
      "Epoch 90/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0631 - val_loss: 1.0303\n",
      "Epoch 91/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0716 - val_loss: 1.2185\n",
      "Epoch 92/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 0.0686 - val_loss: 0.9780\n",
      "Epoch 93/100\n",
      "3842/3842 [==============================] - 20s 5ms/step - loss: 0.0619 - val_loss: 1.1054\n",
      "Epoch 94/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0739 - val_loss: 1.0130\n",
      "Epoch 95/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0607 - val_loss: 1.0641\n",
      "Epoch 96/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0657 - val_loss: 1.0020\n",
      "Epoch 97/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0657 - val_loss: 1.1042\n",
      "Epoch 98/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0557 - val_loss: 0.8810\n",
      "Epoch 99/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0621 - val_loss: 1.4142\n",
      "Epoch 100/100\n",
      "3842/3842 [==============================] - 19s 5ms/step - loss: 0.0656 - val_loss: 0.8848\n",
      "Epoch 1/100\n",
      "2101/3842 [===============>..............] - ETA: 7s - loss: 16.1805Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\SW\\AppData\\Local\\Temp\\ipykernel_18112\\1820761606.py\", line 132, in <module>\n",
      "    history_sog = model_sog.fit(X_train_reshaped, y_sog_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_sog_test))\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 894, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 926, in _call\n",
      "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\", line 143, in __call__\n",
      "    return concrete_function._call_flat(\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\", line 1757, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\", line 381, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"C:\\Users\\SW\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"c:\\Users\\SW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "# LSTM + CNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Conv1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pyproj import Transformer\n",
    "\n",
    "import binascii\n",
    "from shapely import wkb\n",
    "import pyproj\n",
    "\n",
    "def convert_wkb_to_coordinates(wkb_string):\n",
    "    \n",
    "    # 0단계 :Check if the value is a float\n",
    "    if isinstance(wkb_string, float):\n",
    "        return None, None\n",
    "    \n",
    "    # 1단계: 16진수 문자열을 이진 형식으로 디코딩\n",
    "    binary_string = binascii.unhexlify(wkb_string)\n",
    "\n",
    "    # 2단계: 이진 형식 파싱 \n",
    "    geometry = wkb.loads(binary_string)\n",
    "\n",
    "    # 3단계: 좌표를 위도와 경도로 변환\n",
    "    if geometry and hasattr(geometry, 'wkt'):\n",
    "        # Well-Known Text (WKT) 형식으로 변환\n",
    "        wkt = geometry.wkt\n",
    "\n",
    "        # 좌표 시스템 변환 정의\n",
    "        #!crs_from = pyproj.Proj(init='epsg:3857')\n",
    "        #!crs_to = pyproj.Proj(init='epsg:4326')\n",
    "        transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\")\n",
    "        transformer.transform(12, 12)\n",
    "\n",
    "        # 좌표 변환 수행\n",
    "        #! transformed = pyproj.transform(crs_from, crs_to, geometry.x, geometry.y)\n",
    "        transformed = Transformer.transform(transformer,geometry.x, geometry.y)\n",
    "\n",
    "        # 위도와 경도 추출\n",
    "        latitude, longitude = transformed[0], transformed[1]\n",
    "        \n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "# CSV 파일에서 데이터 로드\n",
    "data = pd.read_csv('D:/장우영/LOCALSEARCH/Ship_DA/DA/data/FAmerge_20230607_114714.csv', encoding='ANSI')\n",
    "\n",
    "# 특정 칼럼의 조건에 따라 데이터 필터링\n",
    "filtered_data = data[data['sog'] > 3]  # 특정 칼럼과 조건을 적절히 수정해야 합니다\n",
    "\n",
    "data= filtered_data\n",
    "\n",
    "\n",
    "# 좌표 변환 적용 및 데이터프레임 업데이트\n",
    "data[['latitude', 'longitude']] = data['geom'].apply(convert_wkb_to_coordinates).apply(pd.Series)\n",
    "\n",
    "#print(data[['latitude', 'longitude']])\n",
    "\n",
    "# \"insert_time\"을 숫자로 변환\n",
    "data['year'] = pd.to_datetime(data['insert_time']).dt.year\n",
    "data['month'] = pd.to_datetime(data['insert_time']).dt.month\n",
    "data['day'] = pd.to_datetime(data['insert_time']).dt.day\n",
    "data['hour'] = pd.to_datetime(data['insert_time']).dt.hour\n",
    "data['minute'] = pd.to_datetime(data['insert_time']).dt.minute\n",
    "data['second'] = pd.to_datetime(data['insert_time']).dt.minute\n",
    "\n",
    "\n",
    "# 특성과 타겟 변수 선택\n",
    "X = data[[\"mmsi\", \"ship_type\", \"latitude\", \"longitude\", \"cog\", \"sog\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\", \"풍향\", \"유향\", \"기온\", \"수온\", \"풍속\", \"유속\", \"기압\", \"습도\"]]\n",
    "\n",
    "# 각 변수에 대한 타겟 변수 선택\n",
    "y_latitude = data[[\"latitude\"]]\n",
    "y_longitude = data[[\"longitude\"]]\n",
    "y_cog = data[[\"cog\"]]\n",
    "y_sog = data[[\"sog\"]]\n",
    "\n",
    "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_latitude_train, y_latitude_test, y_longitude_train, y_longitude_test, y_cog_train, y_cog_test, y_sog_train, y_sog_test = train_test_split(X, y_latitude, y_longitude, y_cog, y_sog, test_size=0.2, random_state=42)\n",
    "\n",
    "# MinMaxScaler를 사용하여 입력 특성 스케일 조정\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# LSTM에 맞게 입력 데이터 형태 변환\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# LSTM과 CNN을 결합한 모델 생성\n",
    "model_latitude = Sequential()\n",
    "model_longitude = Sequential()\n",
    "model_cog = Sequential()\n",
    "model_sog = Sequential()\n",
    "\n",
    "# Conv1D 레이어를 추가합니다. 1D 컨볼루션 연산을 수행하는 레이어로, 입력 데이터의 필터 수는 64개, 커널 크기는 3입니다. 활성화 함수로는 ReLU를 사용하고, 입력 형태는 X_train_reshaped의 shape[1]과 shape[2]에 해당합니다.\n",
    "model_latitude.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model_longitude.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model_cog.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model_sog.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "\n",
    "# LSTM은 순환 신경망의 한 종류로, 입력 데이터에 대한 장기 의존성을 모델링하는 데 사용됩니다. 이 레이어의 뉴런 수는 64개\n",
    "model_latitude.add(LSTM(64))\n",
    "model_longitude.add(LSTM(64))\n",
    "model_cog.add(LSTM(64))\n",
    "model_sog.add(LSTM(64))\n",
    "\n",
    "# Dense 레이어를 추가합니다. 출력층으로 사용되며, 뉴런 수는 1개입니다. 각각의 모델에 대해 예측을 수행하므로 출력은 1개\n",
    "model_latitude.add(Dense(1))\n",
    "model_longitude.add(Dense(1))\n",
    "model_cog.add(Dense(1))\n",
    "model_sog.add(Dense(1))\n",
    "\n",
    "# 모델 컴파일\n",
    "model_latitude.compile(loss='mse', optimizer='adam')\n",
    "model_longitude.compile(loss='mse', optimizer='adam')\n",
    "model_cog.compile(loss='mse', optimizer='adam')\n",
    "model_sog.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='min')\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "history_latitude = model_latitude.fit(X_train_reshaped, y_latitude_train, epochs=30, batch_size=32, validation_data=(X_test_reshaped, y_latitude_test))\n",
    "history_longitude = model_longitude.fit(X_train_reshaped, y_longitude_train, epochs=30, batch_size=32, validation_data=(X_test_reshaped, y_longitude_test))\n",
    "history_cog = model_cog.fit(X_train_reshaped, y_cog_train, epochs=30, batch_size=32, validation_data=(X_test_reshaped, y_cog_test))\n",
    "history_sog = model_sog.fit(X_train_reshaped, y_sog_train, epochs=30, batch_size=32, validation_data=(X_test_reshaped, y_sog_test))\n",
    "\n",
    "# 모델을 사용하여 예측 수행\n",
    "y_latitude_pred = model_latitude.predict(X_test_reshaped)\n",
    "y_longitude_pred = model_longitude.predict(X_test_reshaped)\n",
    "y_cog_pred = model_cog.predict(X_test_reshaped)\n",
    "y_sog_pred = model_sog.predict(X_test_reshaped)\n",
    "\n",
    "# Convert predictions to a list\n",
    "y_latitude_pred_list = y_latitude_pred.tolist()\n",
    "y_longitude_pred_list = y_longitude_pred.tolist()\n",
    "y_cog_pred_list = y_cog_pred.tolist()\n",
    "y_sog_pred_list = y_sog_pred.tolist()\n",
    "\n",
    "# 평균 제곱 오차 계산\n",
    "mse_latitude = mean_squared_error(y_latitude_test, y_latitude_pred)\n",
    "mse_longitude = mean_squared_error(y_longitude_test, y_longitude_pred)\n",
    "mse_cog = mean_squared_error(y_cog_test, y_cog_pred)\n",
    "mse_sog = mean_squared_error(y_sog_test, y_sog_pred)\n",
    "\n",
    "print(\"Mean Squared Error - Latitude:\", mse_latitude)\n",
    "print(\"Mean Squared Error - Longitude:\", mse_longitude)\n",
    "print(\"Mean Squared Error - COG:\", mse_cog)\n",
    "print(\"Mean Squared Error - SOG:\", mse_sog)\n",
    "\n",
    "# Create the response JSON\n",
    "response = {\n",
    "    \"latitude\": y_latitude_pred_list[0][0],\n",
    "    \"longitude\": y_longitude_pred_list[0][0],\n",
    "    \"predicted_cog\": y_cog_pred_list[0][0],\n",
    "    \"predicted_sog\": y_sog_pred_list[0][0]\n",
    "}\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 모델 저장\n",
    "with open('model_latitude02.pkl', 'wb') as f:\n",
    "    pickle.dump(model_latitude, f)\n",
    "\n",
    "with open('model_longitude02.pkl', 'wb') as f:\n",
    "    pickle.dump(model_longitude, f)\n",
    "\n",
    "with open('model_cog02.pkl', 'wb') as f:\n",
    "    pickle.dump(model_cog, f)\n",
    "\n",
    "with open('model_sog02.pkl', 'wb') as f:\n",
    "    pickle.dump(model_sog, f)\n",
    "\n",
    "# 모델 불러오기\n",
    "with open('model_latitude02.pkl', 'rb') as f:\n",
    "    model_latitude = pickle.load(f)\n",
    "\n",
    "with open('model_longitude02.pkl', 'rb') as f:\n",
    "    model_longitude = pickle.load(f)\n",
    "\n",
    "with open('model_cog02.pkl', 'rb') as f:\n",
    "    model_cog = pickle.load(f)\n",
    "\n",
    "with open('model_sog02.pkl', 'rb') as f:\n",
    "    model_sog = pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
